

\textbf{Modern neuroscience lacks robust methods to characterise long-duration
and continual time series}, especially in settings where the statistical
properties of the data evolve over time. This limitation present a
methodological gap that must be addressed in order to unlock the scientific
potential of NaLoDuCo experiments.

To bridge this gap, we will develop and disseminate a software library
containing new implementations of machine learning methods specifically
tailored to: (1) operate effectively under \textbf{non-stationary} conditions,
and (2) scale to \textbf{very long time series}.

\subsubsubsection{Initial list of methods to include in the library}
\label{sec:initialListOfMethods}

We will initially populate this library with new implementations of methods
already in use at the GCNU, SWC, and AIND to analyse neural and behavioural time
series from NaLoDuCo foraging and olfactory learning experiments in mice. These
methods span multiple domains—kinematics, neural dynamics, behavioural state
segmentation, forecasting, and joint modelling—and are designed to work together
within an integrated analysis pipeline. We described these methods below and
summarise them in Table~\ref{table:initialMethodsToDisseminate}.

\vspace{1em}
\noindent\textbf{Behavioural Analysis:}  
The first step in behavioural analysis involves multi-body-part tracking. For
this, we will use \textbf{deep learning-based pose estimation} methods such as
\href{https://github.com/talmolab/sleap}{SLEAP}, which enable accurate and
efficient tracking of multiple unmarked mice across long recording sessions.

From the tracked poses, we will infer continuous kinematic variables using
\textbf{linear dynamical systems (LDS)}, including particle filter-based
variants to handle uncertainty and measurement noise. These kinematic features
will be used to infer discrete behavioural states with \textbf{Hidden Markov
Models (HMMs)}, as implemented in tools such as
\href{https://dattalab.github.io/moseq2-website/index.html}{MoSeq}.

We will relate these inferred states and kinematic variables to
foraging-related outcomes—such as patch-leaving decisions—using both
\textbf{generalised linear models (GLMs)} and \textbf{deep neural networks}.
These models will allow us to capture both interpretable and high-capacity
representations of behavioural decision-making processes.

To recover the latent strategies guiding animal behaviour, we will apply
\textbf{inverse reinforcement learning} methods such as
\href{https://github.com/haozhu10015/hiql}{HIQL}, which estimate the underlying
reward functions and policies based on observed actions.

NaLoDuCo recordings uniquely support behavioural forecasting over extended
horizons—ranging from hours to days—far beyond what is feasible in conventional
short-duration experiments. To capitalise on this, we will apply
long-horizon forecasting models using \textbf{recurrent neural networks (RNNs)}
and \textbf{transformer architectures}, which are well-suited to modelling
long-range temporal dependencies.

\vspace{1em}
\noindent\textbf{Neural Data Analysis:}  
Analysis of high-density electrophysiology will begin with \textbf{latent
variable modelling} to reduce the dimensionality of population neural
recordings. We will use both linear and nonlinear latent dynamics models,
including \href{https://github.com/joacorapela/svGPFA}{svGPFA}, which uses
Gaussian processes, and \href{https://snel.ai/resources/lfads/}{LFADS}, a deep
generative model based on recurrent neural networks.

The resulting low-dimensional trajectories will be used to infer discrete
neural states via \textbf{HMMs}, using methods such as
\href{https://github.com/lindermanlab/ssm}{SSM}. For neural activity
forecasting across long duration, we will again employ \textbf{RNNs} and
\textbf{transformers}, which can model complex temporal structure in spiking
activity.

We will also decode the animal’s position from hippocampal spike trains using
\textbf{point-process decoders}, enabling the analysis of spatial coding and
replay phenomena during naturalistic foraging behaviour. We will build on
existing implementations such as
\href{https://github.com/Eden-Kramer-Lab/replay_trajectory_classification}{replay\_trajectory\_classification}.

\vspace{1em}
\noindent\textbf{Joint Neural-Behavioural Modelling:}  
To understand the interactions between neural dynamics and behaviour, we will
use models that extract \textbf{shared latent representations} from both
domains. These models will help reveal how cognitive and behavioural states are
jointly encoded in neural activity.

We will adapt
\href{https://github.com/gatsby-sahani/rpm-aistats-2023}{Recognition-Parameterised
Models (RPM)}, a Bayesian approach developed at the GCNU, which infers latent
variables that explain multiple observation streams through highly nonlinear
relationships. We will also use \href{https://cebra.ai/}{CEBRA}, a
state-of-the-art contrastive learning framework designed for multimodal
representation learning, to discover temporally and semantically aligned
neural-behavioural structure.

\begin{table}
    \caption{Initial data analysis methods to disseminate}
    \label{table:initialMethodsToDisseminate}
    \begin{longtable}{|p{1.9cm}|p{3cm}|p{2.5cm}|p{4.5cm}|}
        \hline
        \textbf{Domain} & \textbf{Functionality} & \textbf{Method} & \textbf{Model Type} \\
        \hline\hline
        behaviour & multi-body-part tracking & SLEAP & deep neural network \\
        \hline
        behaviour & kinematics inference & LDS & linear dynamical system \\
        \hline
        behaviour & kinematics inference & LDS & particle filter \\
        \hline
        behaviour & state inference & SSM & hidden Markov model \\
        \hline
        behaviour & regression &  & generalised linear model \\
        \hline
        behaviour & regression &  & deep neural network \\
        \hline
        behaviour & policy inference & L(M)V-IQL & reinforcement learning \\
        \hline
        behaviour & long-duration forecasting &  & RNN \\
        \hline
        behaviour & long-duration forecasting &  & transformers \\
        \hline\hline
        brain & latents inference & svGPFA & Gaussian processes \\
        \hline
        brain & latents inference & LFADS & RNN \\
        \hline
        brain & state inference & SSM & hidden Markov model \\
        \hline
        brain & long-duration forecasting &  & RNN \\
        \hline
        brain & long-duration forecasting &  & transformers \\
        \hline
        brain & decoding & NA & point-process decoder \\
        \hline\hline
        brain \& behaviour & latents inference & RPM & Bayesian inference + deep neural network \\
        \hline
        brain \& behaviour & latents inference & CEBRA & contrastive learning \\
        \hline
    \end{longtable}
\end{table}

\subsubsubsection{Non-stationarity}
\label{sec:non-stationarity}

Many conventional methods for analysing neural and behavioural time series
assume that the underlying data-generating processes are stationary—that is,
their statistical properties remain constant over time. While this assumption
may be acceptable in short-duration experiments, it breaks down in
long-duration and continual recordings. In such settings, animals
learn and adapt, their internal states and motivations fluctuate, and their
behaviour and physiology are influenced by biological rhythms such as circadian,
ultradian, and infradian cycles. These changes induce non-stationarity in the
data, making models that assume stationarity progressively less reliable or
even obsolete.

To address this challenge, we will adapt and develop methods that are
explicitly designed to operate in non-stationary environments. Our approach
draws on techniques from multiple domains, including adaptive signal
processing, machine learning, and Bayesian inference.

\paragraph{Adaptive Signal Processing.}

The field of adaptive signal processing has produced robust methods for
modelling linear systems with time-varying dynamics~\citep{haykin02}.
%
The recursive least-squares (RLS) algorithm, for example, is a well-known
adaptation of the ordinary least squares algorithm that continuously updates
model parameters to perform linear regression under non-stationary conditions.
%
We will use RLS to study time-varying relations between
behavioural states, as inferred by hidden Markov models, and foraging visit
duration.

\paragraph{Continual Learning.}

The field of continual learning develops adaptive methods for
artificial neural networks.  In classic continual learning, a model learns a
sequence of discrete, well-defined tasks.  But in NaLoDuCo experimentation, as
in many real-world settings there are not specific task boundaries. So methods
that do not require task boundaries are needed. They are studied by the
sub field of task-free continual learning and include online
regularisation (which constrain the update of relevant weights), experience
replay (which maintain a small, representative buffer of past samples) and
ensemble methods (which combine the predictions of multiple individual models
with, for example, different learning rates).
%
We will use these techniques, for example, to train pose tracking models on
month-long continuous recordings.

\paragraph{Adaptive State-Space Models.}

In state-space modelling, the Kalman filter provides a principled way to handle
non-stationary Gaussian processes with drifting mean and covariance. More
flexible approaches are needed when data exhibit abrupt regime shifts or
complex latent dynamics. Switching state-space models, such as Switching Linear
Dynamical Systems (SLDS) and Switching Hidden Markov Models (sHMMs), model
discrete changes in underlying system dynamics. For nonlinear, non-Gaussian
signals, particle filters approximate the posterior distribution through
sequential sampling. Bayesian online learning techniques offer a general
framework for continually updating model parameters as new data arrive.
%
Using these techniques we will build models that robustly infer kinematics
over months.

\paragraph{Concept Drift in Machine Learning.}

In the machine learning literature, non-stationarity is often framed under the
concept of \emph{concept drift}, which refers to changes in the joint
distribution of inputs and outputs over time. Such drift can take various
forms—sudden, gradual, or cyclical (e.g., re-emergence of behavioural patterns
linked to circadian or ultradian rhythms).

Techniques for handling concept drift generally fall into three categories: (1)
\emph{detection methods}, which monitor for significant changes in data
distribution; (2) \emph{adaptation methods}, which incrementally update models
using strategies such as sliding windows, online learning, or ensemble-based
approaches; and (3) \emph{forgetting mechanisms}, which allow models to discard
outdated information while retaining relevant past knowledge.

We will apply techniques from the concept drift literature to models that fall
outside the previous categories of focus (e.g., linear models, artificial neural
networks, and state-space models). In particular, we will explore their use in
building
\href{https://github.com/gatsby-sahani/rpm-aistats-2023}{Recognition-Parameterised
Models (RPMs)} to estimate joint behavioural and neural latent variables over
timescales of weeks to months.

In summary, robust analysis of NaLoDuCo datasets requires models that
continuously adapt to evolving data distributions. Our offline analysis
framework will integrate both established adaptive algorithms and cutting-edge
methods from continual learning and concept drift to meet this challenge.

\subsubsubsection{Computational efficiency}

Neural and behavioural data analysis is most effective when computations are
performed quickly, ideally in real time. Slow computations discourage data
exploration and hinder scientific discovery. The large dataset sizes generated by
NaLoDuCo experimentation pose a significant challenge for fast data analysis.

To overcome this limitation, we will combine distributed and GPU computing.
%
Distributed computing is a paradigm
in which tasks and data are divided across multiple computers. Instead of
relying on a single powerful machine, distributed computing accelerates
processing by executing multiple parts of a computation in parallel.
%
GPU computing is a parallel computing approach that uses Graphics Processing
Units (GPUs) to accelerate computational tasks. Unlike traditional Central
Processing Units (CPUs), which execute a few complex operations sequentially,
GPUs consist of thousands of smaller cores optimised for executing many
operations simultaneously.
%
% Distributed and GPU computing address different bottlenecks in large-scale
% computation. GPUs are highly efficient at parallelizing operations within a
% single machine. They excel at matrix operations and batch processing. However,
% GPUs are limited by memory and cannot scale indefinitely when dealing with huge
% datasets that exceed the GPU memory.
%
Distributed computing allows to split workload across multiple machines,
overcoming memory and computational limitations. It is particularly useful for
scaling to massive datasets.

For distributed computing to deliver substantial speed improvements,
computations must be decomposable into independent parallel tasks. Due to their
serial dependencies, time series models are difficult to decomposed in this
manner.
%
Still, time series models can benefit from distributed computing
infrastructures, as many parts of time series pipelines are parallelizable,
like preprocessing steps (e.g., filtering, feature extraction, normalisation)
or parallel model evaluation across hyperparameter sweeps.
%
In addition,  when datasets are too large to fit in memory, distributed
computing (e.g., with Ray, Dask, or Spark) can Distribute I/O and
preprocessing, train models in parallel on different subsets (e.g., one model
per animal or time window) and run hyperparameter sweeps or model variants in
parallel.
%
Furthermore, even with serial dependencies GPU acceleration significantly
speeds up the processing of each item in the time series, specially when large
matrix operation are involved.

% distributed computing for time series models

% parallel version of the Kalman filter

We will develop accelerated implementations of all methods in the library.
These implementations will use \href{https://docs.jax.dev/}{JAX} for model
learning, inference, and numerical computation,
\href{https://spark.apache.org/}{Apache Spark} or
\href{https://www.dask.org/}{Dask} to distribute pre-processing and feature
extraction, and \href{https://docs.ray.io/}{Ray} to distribute machine learning
and deep learning functionality.

Related to this item is the library
\href{https://github.com/thunder-project/thunder}{Thunder}, which accelerate the
analysis of large scale neural data. It was pioneering by introducing the use
of distributed computing in neural data analysis.
%
Our library is different from
\href{https://github.com/thunder-project/thunder}{Thunder} in that, besides
analysing large scale neural data, it processes continual recordings, and needs
to overcome non-stationarity problems.
%
% In addition, it includes methods to characterize behavior, while Thunder
% focuses on neural activity.
%
In addition, \href{https://github.com/thunder-project/thunder}{Thunder}
implements simpler methods assuming independent and identically distributed
data, while our library contains more sophisticated time series ones.

\subsubsubsection{Deliverables}
\label{sec:deliverablesOfflineAnalysis}

\begin{enumerate}

    \item repository containing implementations of machine learning algorithms
        for offline processing NaLoDuCo experimental data, adapted to operate
        in non-stationary environments, and optimised to perform at scale when
        running on public clouds or institutional
        high-performance-computing clusters.

    \item SWC NaLoDuCo foraging datasets and AIND NaLoDuCo odour learning
    datasets stored in DANDI.

    \item deployment of the methods in 1 in Amazon EC2 instances, to allow
        users to analyse on the cloud the datasets in 2.

\end{enumerate}

