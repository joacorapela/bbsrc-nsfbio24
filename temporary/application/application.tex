
\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{verbatim}
\usepackage[colorlinks=true]{hyperref}
\usepackage{longtable}
\usepackage{natbib}

\title{Enabling naturalistic, long-duration and continual animal experimentation}

\begin{document}

\maketitle

\tableofcontents
\pagebreak

\section{Vision}

For over four years, at the Sainsbury Wellcome Centre and Gatsby Computational
Neuroscience Unit, we have been developing the AEON platform, a set of hardware
and software tools that support a new type of experimentation, where animals
are allowed to express ethologically-relevant behaviours, in naturalistic
environments, and in long-duration experiments, while their behaviour and
neural activity is monitored continuously for weeks to months.
%
We have used this platform to characterize foraging behaviour in both solitary
and groups of mice~\citep{aeonRepo} (Figure~\ref{fig:aeonPlatform}).

Our US partner, the Allen Institute for Neural Dynamics, is using the AEON
platform in continuous learning experiments, where mice freely explore odors
continuously for days to weeks~\citep{carlsPapers}.

This is an unprecedented type of experimentation that \ldots

Several groups around the world are performing this new type of
experimentation~\cite{}.

We have built the AEON platform, and have used it to collect weeks- to
months-long NaLoDuCo experimental data. We next propose to develop advanced
machine learning methods and intelligent visualisations to extract meaning from
this data (Aim 1).

A central aim of both the SWC/GCNU and AIND is to contribute to open science.
We thus propose to create software infrastructure to openly disseminate
NaLoDuCo recordings, visualisation and data analysis methods (Aim 2).

Over more than four years we have developed the AEON platform following high
software engineering practices. It is an open source platform that anybody can
use and modify~\citep{aeonResources}. We want it to become the standard platform for the collection
of NaLoDuCo experimental data.
%
We are currently using AEON on two new NaLoDuCo experiments: (1) odor learning
experiments, lasting for days to weeks, lead by Dr.~Carl Schoonover at the
AIND, and (2) foraging experiments in very large arenas (eight meters in
diameter), lead by Prof.~Tiago Branco at the SWC.
%
We will extend and validate the functionality of the AEON platform by applying
it to these and new NaLoDuCo experiments.
%
A key functionality that we propose to add as part of this project is real-time
machine learning, to allow to control AEON experiments with live inferences
(Aim 3).

\begin{figure}
    \begin{center}
        \includegraphics[width=5in]{figures/aims.png}
    \end{center}
    \caption{Proposal aims}
    \label{fig:aims}
\end{figure}

\subsection*{Aim 1: create infrastructure for open dissemination of NaLoDuCo
experimental recordings}

The dissemination of NaLoDuCo recordings is not
trivial, as datasets generated by this new type of experimentation are
enormous. For instance, the size of a dataset generated from a one week
recording of behavioural and neural activity from a foraging mouse in SWC
experiments exceeds 200 terabytes. It will take users several days to download
these datasets over standard Internet connections.

Instead of bringing data to users, we will bring users to data, by storing
datasets in the cloud (or in institutional clusters), and providing
\textbf{cloud software to allow users to visually explore and statistically
analyse behavioural and neural NaLoDuCo datasets where they live}
(Figure~\ref{fig:aims}, left box).

Our statistical analysis of neural time series will require knowledge of the
spiking activity of single units; i.e., spike sorting. In long-duration
experiments with freely moving animals spike sorting is a challenging problem,
because movements of recording probes change the shape of spike waveforms over
time and complicate the assignment of spikes to units based on their waveforms.
We will address this problem by developing \textbf{spike sorting methods for
long-duration and continual, long-duration and high-channel-count recordings}
(Figure~\ref{fig:aims}, left box).

\subsection*{Aim 2: create real-time machine learning methods for intelligent
experimentation}

In small-animal Neuroscience, most often statistical processing of neural time series is
performed offline;
i.e., experimental data is collected, saved to files, which are later
statistically processed, with no runtime constraints. Most often all
experimental data is processes at the same time; i.e., batch processing.

A new online statistical processing approach is now emerging in small-animal Neuroscience,
where data is processes while it is being collected, and at the speed of data
generation~\citep{vermaniEtAl24}.

Online methods are well suited for NaLoDuCo experimentation. In experiments
extending for weeks to months animals learn and adapt, their motivation and
fatigue may fluctuate, and experimental conditions (e.g., lighting) may change.
Offline batch processing algorithms cannot model this type of changing data.
They assume stationary data whose statistical properties do not change across
time. Differently, most online processing algorithms are robust to
these changes.
%
Also, NaLoDuCo experimentation is well suited for online methods, as the
long-duration of these experiments provide a large amount of data to accurately
fit expressive online methods.

We will \textbf{optimize methods developed for Aim~1 so that they can operate
in real time}, and focus on the following two applications of these online
methods (Figure~\ref{fig:aims}, right box).

\subsubsection*{Intelligent neuromodulation}

Brain activity can be modulated optically, chemically and
electrically~\citep{}.
%
Most commonly this modulations is done at fixed experimental times, or based on
simple behavioural or neural observations.

We will guide optogenetic manipulations based on inferences from advanced
machine learning methods.
%
For example, a scientists may hypothesize that a peak in a neural latent
variable, inferred from a prefrontal cortex population, signals the moment when
mice decide to begin a foraging bout.  To test this, she runs an online machine
learning model to estimate latent variables from prefrontal cortex activity,
predicting when this peak will occur. She then optogenetically inactivates the
neural population at the forecasted time.  Because inactivation prevented the
mouse from initiating a foraging bout, her hypothesis is supported.

\subsubsection*{Intelligent experimental data storage}

As the duration of NaLoDuCo experiments become longer, and the richness of the
behavioural and neural recordings become larger, it will be unfeasible to
store all raw data. We will be forced to intelligently decide, in real time,
subsets of data to discard.

For instance, if we are recording videos from a mouse foraging in a large arena
with ten high-resolution cameras, it would save considerable storage if at any
time we only save videos from cameras capturing the mouse at that time.  This
could be done by tracking the position of the mouse in real time with
probabilistic machine learning methods. Then, when the confidence of the
tracking is high, we would only save videos of cameras capturing the mouse at
the tracked position, but when the confidence is low, we would save all videos.

\section{Approach}

We have collected unprecedented NaLoDuCo datasets at the SWC. However, these
very large datasets are of not much use without methods to visualise and
analyse them.
Sections~\ref{sec:offlineAnalysisMethods}
and~\ref{sec:visualExploration}
present such methods.

\subsection{Offline analysis methods}
\label{sec:offlineAnalysisMethods}

We will disseminate an open-source library of advanced statistical methods
adapted to \textbf{efficiently} process \textbf{non-stationary} recordings from
NaLoDuCo experiments.
%
We will benchmark the performance of these methods for processing behavioural
and neural time series recorded from the SWC NaLoDuCo foraging experiments.

The initial list of methods to include in this library is given in
Section~\ref{sec:initialListOfMethods} and includes regression, classification,
supervised, unsupervised and reinforcement learning, state space models,
artificial neural networks and transformers.
%
Implementations of these methods will follow high software engineering
standards, and will include detailed documentation, so that users can easily
apply them to process their own NaLoDuCo datasets.
%
% If a user needs a machine learning method to process NaLoDuCo recordings not
% included in the library, she could adapt similar methods
% included in the library, and their corresponding documentation, to adapt this
% algorithm to efficiently process her recordings.
%
This library should be built for all and by all, and become essential to the
rapidly expanding community of scientists performing NaLoDuCo experimentation
in small animals around the world~\citep{}.

In Neuroscience we don't have methods to characterise long-duration and
continual time series, to learn from time series whose statistical properties
fluctuate over time, to forecast time series over long horizons (e.g., hours,
days, week or months).
%
This library should find applicability beyond the realm of small-animal
neuroscience and, for example, be useful to characterize long-duration and
continual neural processes in human brain activity measured with subscalp EEG
electrodes~\citep{}.

\subsubsection{Initial list of methods to include in the library}
\label{sec:initialListOfMethods}

The first step in the analysis of NaLoDuCo foraging behavioural data is
\textbf{tracking multiple body parts} in mice. For this we will use
\textbf{deep learning} methods, as in
\citep{https://pubmed.ncbi.nlm.nih.gov/30127430/}.
%
Next, we will use the previous tracking outputs to \textbf{infer mice
kinematics} with \textbf{linear dynamical models}, as in
\citep{https://github.com/joacorapela/lds}.
%
We will combine the tracking outputs with the kinematics inferences to
\textbf{infer behavioural states} with \textbf{hidden Markov models}, as in
\citep{https://pubmed.ncbi.nlm.nih.gov/26687221/}.
%
Further, we will \textbf{related kinematics and behavioural states to the
probability of foraging events}, like leaving a patch, with \textbf{generalized
linear models}, as in \citep{}, and \textbf{artificial neural networks}, as in \citep{}.
%
The final step of the behavioural analysis will be to \textbf{infer mice policy
from behavioural measures} with \textbf{inverse reinforcement learning}, as in
\cite{https://arxiv.org/abs/2311.13870v2}.

The characterization of neural data will begin with the \textbf{estimation of latent
variables models}, to reduced the dimensionality of multielectrode recordings of
hundreds or even thousands of neurons to a small number of latent variables,
using \textbf{latent variable models}, with linear~\citep{dunckerAndSahanai18}
and nonlinear~\citep{pandarinathEtAl} latent dynamics..
%
We will use the estimated latent variables as inputs to
\textbf{infer neural states}, using \textbf{HMMs}, as in~\citep{}.
%
Next, we will \textbf{decode mice position} from hippocampal recordings, and
\textbf{study
replay} during long-duration \textbf{foraging}, with \textbf{point process
decoders}, as in \citep{ppDecoder}.

\begin{longtable}{|p{1.9cm}|p{3cm}|p{2.5cm}|p{4.5cm}|}
    \hline
    \textbf{Domain} & \textbf{Functionality} & \textbf{Method} & \textbf{Model} \\
    \hline
    behaviour & multi-body-part tracking & SLEAP & deep neural network \\
    \hline
    behaviour & kinematics inference & lds\_python & linear dynamical system \\
    \hline
    behaviour & kinematics inference & lds\_python & particle filter \\
    \hline
    behaviour & behavioural state inference & SSM & hidden Markov model \\
    \hline
    behaviour & behavioural predictions & ??? & generalized linear model \\
    \hline
    behaviour & behavioural predictions & ??? & deep neural network \\
    \hline
    behaviour & policy inference & L(M)V-IQL & reinforcement learning \\
    \hline
    behaviour & forecasting behaviour & ??? & RNN \\
    \hline
    behaviour & forecasting behaviour & ??? & transformers \\
    \hline
    brain & latents inferences & svGPFA & Gaussian processes \\
    \hline
    brain & latents inferences & LFADS & RNN \\
    \hline
    brain & neural state inferences & SSM & hidden Markov model \\
    \hline
    brain & decoding & NA & point-process decoder \\
    \hline
    brain \& behaviour & latents inference & RPM & Bayesian inference + deep neural network \\
    \hline
    brain \& behaviour & latents inference & CEBRA & contrastive learning \\
    \hline
\end{longtable}

\subsubsection{Challenges}

Extracting meaning from long-duration and continual recordings opens
challenges and opportunities that we will address and exploit in this project,
as we describe in this and the next sections.

\subsubsection*{Non-stationarity}

Conventional offline methods used to characterize neural time series assume
that the statistical characteristics of the modeled data do not change with
time (i.e., that the probability of the data is time invariant --
stationarity). This assumption may be valid for shorter experiments. However,
for long-duration experiments, where animals learn and adapt, where their
motivation fluctuates, and their activity is modulated by circadian, utradiem
and peridiem rhythms, this assumption may not hold. In nonstationary
environments, a non-adaptive model trained under the false stationarity
assumption is bound to become obsolete in time, and perform sub-optimally at
best, or fail catastrophically at worst.
%
Below we briefly describe the type of methods we will use to adapt the
disseminated methods to non-stationary environments.

The field of adaptive signal processing develops algorithms to characterize
non-stationary systems~\citep{haykin02}. In this field adaptations to specific
algorithms have been developed to improve their performance in non-stationary
environments.

For example, the recursive least-squares algorithm \citep[][Chapter
9]{haykin02} is an adaptation of the ordinary least square algorithm to perform
\textbf{linear regression} with non-stationary data.

For non-linear regression using \textbf{artificial neural networks}, a very large number
of strategies have been developed to address data non-stationarity. To mention
a few, continual learning has introduced algorithms like  Elastic Weight
Consolidation~\citep[EWC][]{} and Learning Without Forgetting~\citep[LwF][]{}
to allow models to adapt to changes over time without catastrophic forgetting.
Also from this subfield is the Experience Replay (ER) algorithm that stores
past data samples in a buffer and replays them alongside new data during
training. A different type of strategy is used by ensemble methods~\citep{},
which combine multiple models trained on different time windows to capture
evolving data patterns.

Algorithms for \textbf{state-space models}, such as the Kalman filter, perform well in
relatively simple non-stationary environments where data exhibit a Gaussian
distribution with time-varying mean and covariance. However, in more complex
settings with abrupt regime shifts or structured variability, more flexible
approaches are required.

Switching state-space models, such as the switching linear dynamical system
(SLDS) and the switching Hidden Markov model (sHMM), address discrete changes
in system dynamics by adapting to different latent states. For tracking
nonlinear and non-Gaussian processes, particle filters offer a powerful
alternative by approximating posterior distributions through sequential
sampling. Additionally, Bayesian online learning provides a principled
framework for adapting probabilistic models to evolving data distributions,
enabling continual adaptation in dynamic environments.

% RL

% concept drift
In the machine learning literature the study of non-stationary systems is done
under the label of \textbf{concept drift}~\citep{}, which refers to a change in
the statistical properties of data that causes a model to perform poorly.
%
Differently from adaptive signal processing, most methods developed to tackle
concept drift are model agnostic and can be used with multiple machine learning
models.

Concept drift can happen suddenly or gradually, and follow a periodic pattern
where old concepts periodically reappear (e.g., circadian rhythm variations in
neural firing rates). In such scenarios algorithms should remember previous
contexts and re-instate them as soon as they reappear, overcoming catastrophic
forgetting.

A basic strategy to address concept drift is to test for data distribution
changes in data windows and retrain or update models when changes are detected.
Several options exist for testing for distributional changes and for performing
model updates.
%
Alternatively, one could use ensemble methods that combine multiple models to
mitigate the negative effects of drift (e.g., combine classifiers with
different learning rates and weight them according to their accuracy.
%
Most concept drift methods are designed for supervised learning, but methods
such as clustering evolution, density estimation changes, and autoencoder-based
monitoring can detect drift without labeled data.

\subsubsection*{Processing times for very large datasets}

Neural and behavioral data analysis is most effective when computations are
performed quickly, ideally in real time. Slow computations discourage data
exploration and hinder scientific discovery. The large dataset sizes generated by
NaLoDuCo experimentation pose a significant challenge for fast data analysis.

To overcome this limitation, we will combine distributed and GPU computing.
%
Distributed computing is a paradigm
in which tasks and data are divided across multiple computers. Instead of
relying on a single powerful machine, distributed computing accelerates
processing by executing multiple parts of a computation in parallel.
%
GPU computing is a parallel computing approach that uses Graphics Processing
Units (GPUs) to accelerate computational tasks. Unlike traditional Central
Processing Units (CPUs), which execute a few complex operations sequentially,
GPUs consist of thousands of smaller cores optimized for executing many
operations simultaneously.

Distributed and GPU computing address different bottlenecks in large-scale
computation. GPUs are highly efficient at parallelizing operations within a
single machine. They excel at matrix operations and batch processing. However,
GPUs are limited by memory and cannot scale indefinitely when dealing with huge
datasets that exceed the GPU memory.
%
Distributed computing allows to split workload across multiple machines,
overcoming memory and computational limitations. It is particularly useful for
scaling to massive datasets (e.g., long-term time series recordings).

\begin{comment}

For distributed computing to deliver substantial speed improvements,
computations must be decomposable into independent parallel tasks. Due to their
serial dependencies, time series models are difficult to decomposed in this
manner.
%
Still, time series models can benefit from distributed computing
infrastructures.

While not
all algorithms naturally support such decomposition, in machine learning
distributed computing remains valuable even for non-parallelizable algorithms.
For instance, it enables efficient hyperparameter optimization, where multiple
algorithm configurations can be tested in parallel, significantly reducing
overall runtime.

% distributed computing for time series models

% parallel version of the Kalman filter

\end{comment}

We will develop accelerated implementations of all methods in the library of
methods to process NaLoDuCo experimental data
(Section~\ref{sec:offlineAnalysisMethods}). These implementations will use
JAX\footnote{\url{https://docs.jax.dev/}} for model learning, inference, and numerical computation, Apache
Spark\footnote{\url{https://spark.apache.org/}} or
Dask\footnote{\url{https://www.dask.org/}} to distribute pre-processing and
feature extraction, and Ray\footnote{\url{https://docs.ray.io/}} to distribute
machine learning and deep learning functionality.

Thunder~\cite{} is a library developed in 2014 to accelerate the analysis of
large scale neural data. It was pioneering by introducing the use of
distributed computing in neural data analysis.
%
Our library is different from Thunder in that, besides analyzing large scale
neural data, it processes continual recordings, and needs to overcome
non-stationarity problems.
%
In addition, it includes methods to characterize behavior, while
Thunder focuses on neural activity.
%
Finally, Thunder implements simpler methods assuming independent and
identically distributed data, while our library contains more sophisticated
time series ones.

\subsubsection{Opportunities}

\paragraph{More expressive models}
%
Our long-duration recordings, spanning weeks to months and generating hundreds
of terabytes per experiment, will be transformative for neuroscience, much like
the advent of large-scale datasets in computer vision.  Just as the creation of
MNIST—and later, ImageNet—enabled the training of deeper neural networks,
leading to unprecedented performance breakthroughs, our massive,
high-resolution neural and behavioural datasets will allow the estimation of far
more expressive models than previously possible.
%
For instance, large NaLoDuCo datasets will allow to estimate latent variable
models with highly nonlinear and expressive models of observation given latent
variables using recognition parametrised models~\citep{walkerEtAl23}.
%
By capturing neural dynamics over extended timescales, we may uncover novel
insights into learning, memory, and long-term neural adaptations that remain
inaccessible with conventional short-duration studies.

\paragraph{Forecasting over much longer horizons}
%
Short-duration studies often fail to capture dependencies that unfold over long
timescales. In contrast, NaLoDuCo experimental data allow forecasting models to
incorporate past events spanning hours to weeks, enabling more reliable
predictions of behavioral and neural activity over extended horizons.
%
For example, forecasting models trained on NaLoDuCo data could predict upcoming
behavioral transitions, such as shifts in movement patterns, attention, or task
engagement, hours in advance—a capability that is unattainable with
shorter-duration experiments.

\paragraph{Study very slow behavioural and neural rhythmic patterns}
%
Continuously monitoring detailed behavioural and neural activity over weeks to
months enables the study of slow rhythmic processes that extend beyond
traditional circadian (~24-hour) rhythms, including ultradian (hours),
infradian (days to weeks), and even multi-month cycles. These long-duration
fluctuations influence learning, memory consolidation, motivation, and
cognitive function, yet they remain largely unexplored in controlled
experiments. By capturing these dynamics, we can gain new insights into neural
plasticity, attention, and mood regulation, as well as the progression of
neurological disorders like Parkinson’s disease and depression, which exhibit
slow symptom fluctuations.

\paragraph{New neuromodulation opportunities}
%
In traditional short-duration experiments, the effects of neuromodulation are
tested immediately. In contrast, our long-duration experiments will enable
repeated neuromodulation over extended periods and allow us to assess its
impact over much longer timescales.
%
For instance, in a mouse model of Alzheimer's disease, we could apply
optogenetic stimulation to the hippocampus for one hour per day over the course
of a month and assess its impact on memory retention and synaptic plasticity in
the following weeks. This approach could reveal whether intermittent
neuromodulation promotes long-term neural circuit stability and delays
cognitive decline.

\subsubsection{Related research}
\label{sec:relatedResearchOfflineAnalysis}

\paragraph{Neural data analysis methods from the Gatsby Unit}
%
The Gatsby Unit has developed world-class neural data analysis methods for
%
inferring latent variables using Gaussian
processes~\citep{yuEtAl09,dunckerAndSahani18,ruttenEtAl20,yuEtAl24}, or variants
of linear dynamical systems~\citep{buesinEtAl12,mackeEtAl15}, or recognition
parameterised models~\citep{walkerEtAl23},
%
for separating contributions of different factors to spiking activity using
tensor decompositions~\citep{soulatEtAl21},
%
and for understanding the effects of neural perturbations~\citep{oSheaEtAl22},
just to mention a few.

\paragraph{Distributed computing for small animal Neuroscience}
%
\citet{freemanEtAl14} introduced a
package\footnote{https://github.com/thunder-project/thunder} for analysing
two-photon imaging records on distributed computing platforms.  This has been a
pioneering development by introducing distributing computing into neuroscience
data analysis. However, it used short duration imaging recordings and it
implemented simple data analysis methods.

\paragraph{Continuous epilepsy monitoring}
%
Continuous epilepsy monitoring is a transformative technology for diagnosing,
understanding, and managing epilepsy. By capturing long-term brain activity, it
provides crucial insights into seizure patterns and underlying neural dynamics.

A major advancement in this field is the development of implantable devices,
such as the NeuroPace Responsive Neurostimulation (RNS) system, which
continuously records electrocorticographic (ECoG) brain activity over extended
periods (e.g., years). In addition to monitoring, RNS delivers targeted
electrical stimulation in response to detected seizure precursors,
significantly improving epilepsy treatment outcomes \citep{rao21}. However, RNS
has two key limitations: (1) it is invasive, requiring surgical implantation,
and (2) it can only store a limited amount of brain activity (typically a few
hours) for later analysis, restricting its utility for studying long-term
neural dynamics.

To overcome these challenges, researchers have developed subscalp ultra-long
EEG recording technologies \citep{duunHenriksenEtAl20}, which use electrodes
implanted under the scalp but above the skull. This approach is less invasive
than intracranial devices and offers continuous EEG monitoring, with the added
benefit of streaming data directly to the cloud. This capability enables
long-term characterization of brain activity.

Subscalp EEG monitoring has already provided valuable insights into epilepsy.
For example, it has revealed that seizure susceptibility is often modulated by
circadian and ultradian rhythms, with specific times of day associated with
increased seizure likelihood. However, compared to the AEON platform, subscalp
EEG provides lower-resolution measurements of both neural and behavioural
activity. Additionally, the data modeling methods used in subscalp EEG research
remain largely proprietary, as much of this technology is developed by private
companies. Furthermore, current implementations are primarily designed for
seizure detection and forecasting, rather than for broader investigations into
long-term brain dynamics.

\subsubsection{Outputs}
\label{sec:outputsOfflineAnalysis}

\begin{enumerate}

    \item repository containing implementations of machine learning algorithms
        for offline processing NaLoDuCo experimental data, adapted to operate
        in non-stationary environments, and optimized to perform at scale when
        running on public clouds or institutional
        high-performance-computing clusters.

    \item SWC NaLoDuCo foraging dataset storred in DANDI.

    \item deployment of the methods in 1 in Amazon EC2 instances, to allow
        users to analyze on the cloud the datasets in 2.

\end{enumerate}

\subsection{Visual Exploration}
\label{sec:visualExploration}

\subsubsection{Outputs}

\begin{enumerate}

    \item visualisations for continuous behavioural and neural recording

    \item visualisations for epoched behavioural and neural recording

    \item visualisations for model outputs

    \item indexing system to support intelligent visualisations

    \item deployment of the above items to allow users to visualise NaLoDuCo
        DANDI datasets on the cloud

\end{enumerate}

\subsection{Spike Sorting}

\subsubsection{Outputs}

\begin{enumerate}

    \item Repository with implementations and benchmarking of offline spike
        sorting algorithms for long-duration recordings

    \item Repository with implementations and benchmarking of online spike
        sorting algorithms

\end{enumerate}

\begin{comment}
\subsection{Dissemination}
\label{sec:dissemination}

\subsubsection{Outputs}

- web application for the visualisation and analysis of SWC foraging NaLoDuCo recordings in AWS
- documentation on how to build arenas and use the AEON platform and software
- documentation on how to use the online machine learning software
\end{comment}

\subsection{Online Machine Learning}

\subsubsection{Outputs}

\begin{enumerate}

    \item Bonsai packages implementing real-time ML functionality for experimetal control

    \item Documentation of these packages

\end{enumerate}

% \subsection{Software and Infrastructure}
% Python, Kubernetes, Apache Spark, Ray, Bonsai, Bonsai.ML

\end{document}
