
Modern neuroscience lacks robust methods to characterize \textit{long-duration
and continual time series}, especially in settings where the statistical
properties of the data evolve over time. Furthermore, few approaches are
currently capable of performing accurate \textit{long-horizon forecasting}—over
hours, days, or weeks—on behavioral or neural data. These limitations present a
methodological gap that must be addressed in order to unlock the scientific
potential of NaLoDuCo experiments.

To bridge this gap, we will develop and disseminate software libraries that
implement offline analysis methods specifically tailored to: (1) operate
effectively under \textbf{non-stationary conditions}, and (2) scale to
\textbf{very long time series}. All implementations will be optimized for
high-throughput, distributed computation on large datasets.

\subsubsection*{Initial List of Methods to Include in the Library}
\label{sec:initialListOfMethods}

We will initially populate this library with the methods already in use at
GCNU, SWC, and AIND to analyze neural and behavioral data in NaLoDuCo foraging
and olfactory learning experiments in mice. These methods span multiple
domains—kinematics, neural dynamics, behavioral state segmentation,
forecasting, and joint modeling—and are grouped below by function.

\vspace{1em}
\noindent\textbf{Behavioral Analysis:}  
The first step in behavioral analysis involves multi-body-part tracking. For
this, we will use \textbf{deep learning-based pose estimation} methods such as
\href{https://github.com/talmolab/sleap}{SLEAP}, which allow tracking of
multiple unmarked mice across long durations.

Next, we will infer continuous kinematic variables from these pose estimates
using \textbf{linear dynamical systems (LDS)}, including variants based on
particle filters (see \href{https://github.com/joacorapela/lds}{LDS}). These
features will be used to infer discrete behavioral states with \textbf{Hidden
Markov Models (HMMs)}, as implemented in tools such as
\href{https://dattalab.github.io/moseq2-website/index.html}{MoSeq}.

We will relate behavioral kinematics and inferred states to foraging-related
outcomes (e.g., patch-leaving decisions) using both \textbf{generalized linear
models (GLMs)} and \textbf{deep neural networks}. For long-horizon prediction
of behavioral trajectories, we will implement \textbf{recurrent neural networks
(RNNs)} and \textbf{transformer architectures}. To infer behavioral policies,
we will apply \textbf{inverse reinforcement learning} methods such as
\href{https://github.com/haozhu10015/hiql}{HIQL}.

\vspace{1em}
\noindent\textbf{Neural Data Analysis:}  
Analysis of high-density electrophysiology will begin with \textbf{latent
variable modeling} to reduce the dimensionality of multi-electrode recordings.
We will use both linear and nonlinear approaches, including
\href{https://github.com/joacorapela/svGPFA}{svGPFA} (a Gaussian Process latent
dynamical model) and \href{https://snel.ai/resources/lfads/}{LFADS} (a deep
generative model based on RNNs).

The latent trajectories will be used as inputs to infer discrete neural states
via \textbf{HMMs}, using libraries such as
\href{https://github.com/lindermanlab/ssm}{SSM}. For long-duration forecasting
of neural activity, we will again employ \textbf{RNNs} and
\textbf{transformers}.

We will also perform spatial decoding from hippocampal spikes using
\textbf{point-process decoders}, enabling the study of long-term replay during
naturalistic foraging, as implemented in
\href{https://github.com/Eden-Kramer-Lab/replay_trajectory_classification}{replay\_trajectory\_classification}.

\vspace{1em}
\noindent\textbf{Joint Neural-Behavioral Modeling:}  
To better understand the relationships between neural and behavioral processes,
we will develop models that extract \textbf{shared latent variables} from both
domains. This includes
\href{https://github.com/gatsby-sahani/rpm-aistats-2023}{Recognition-Parametrized
Models (RPM)}, a method developed at the GCNU for joint inference from multiple
modalities, and \href{https://cebra.ai/}{CEBRA}, a recent contrastive learning
approach for multimodal representation learning.

\subsubsection*{Non-stationarity}

Many conventional methods for analyzing neural and behavioral time series
assume that the underlying data-generating processes are stationary—that is,
their statistical properties remain constant over time. While this assumption
may be acceptable in short-duration experiments, it breaks down in
long-duration and continual recordings. In such settings, animals
learn and adapt, their internal states and motivations fluctuate, and their
behavior and physiology are influenced by biological rhythms such as circadian,
ultradian, and infradian cycles. These changes induce non-stationarity in the
data, making models that assume stationarity progressively less reliable or
even obsolete.

To address this challenge, we will adapt and develop methods that are
explicitly designed to operate in non-stationary environments. Our approach
draws on techniques from multiple domains, including adaptive signal
processing, machine learning, and Bayesian inference.

\paragraph{Adaptive Signal Processing.}  

The field of adaptive signal processing has produced robust methods for
modeling linear systems with time-varying dynamics~\citep{haykin02}.
%
The recursive least-squares (RLS) algorithm, for example, is a well-known
adaptation of the ordinary least squares algorithm that continuously updates
model parameters to perform linear regression under non-stationary conditions.
%
We will use generalized versions of RLS to study time-varying relations between
behavioral states, as inferred by hidden Markov models, and mice decisions,
like when to leave a foraging patch.

\paragraph{Neural Networks and Continual Learning.}  

For nonlinear models such as artificial neural networks, a wide range of
strategies have been proposed to handle non-stationarity. The subfield of
\emph{continual learning} has introduced techniques such as Elastic Weight
Consolidation (EWC) and Learning Without Forgetting (LwF), which preserve
performance on previously learned tasks while adapting to new data. Experience
Replay (ER), another technique from this domain, maintains a buffer of
historical data that is replayed during training to avoid catastrophic
forgetting. Ensemble methods further enhance robustness by combining models
with different adaptability profiles, for example, models trained on different
time windows or with varied learning rates.

\paragraph{Adaptive State-Space Models.}  

In state-space modeling, the Kalman filter provides a principled way to handle
non-stationary Gaussian processes with drifting mean and covariance. More
flexible approaches are needed when data exhibit abrupt regime shifts or
complex latent dynamics. Switching state-space models, such as Switching Linear
Dynamical Systems (SLDS) and Switching Hidden Markov Models (sHMMs), model
discrete changes in underlying system dynamics. For nonlinear, non-Gaussian
signals, particle filters approximate the posterior distribution through
sequential sampling. Bayesian online learning techniques offer a general
framework for continually updating model parameters as new data arrive.

\paragraph{Concept Drift in Machine Learning.}  

In the machine learning literature, non-stationarity is often studied under the
framework of \emph{concept drift}~\citep{conceptdriftReview}, which refers to
shifts in the joint distribution of inputs and outputs over time. Unlike
adaptive signal processing, many concept drift techniques are model-agnostic
and can be layered on top of various learning architectures. Drift may be
sudden, gradual, or cyclical (e.g., re-emergence of behavioral patterns linked
to circadian modulation). Effective strategies involve drift detection followed
by targeted model updates or retraining. Ensemble approaches can also be used
to mitigate drift by weighting component models according to recent predictive
accuracy.

Although many concept drift methods have been developed for supervised
learning, unsupervised extensions exist, including drift detection via
clustering evolution, density monitoring, or changes in autoencoder
reconstruction loss. These tools are particularly relevant in our setting,
where labeled data may be sparse or unavailable.

In summary, robust modeling of NaLoDuCo datasets demands tools that adapt
continuously to changing data distributions. Our offline analysis framework
will incorporate both well-established adaptive algorithms and cutting-edge
methods from continual learning and concept drift research to address this
fundamental challenge.

\subsubsection*{Computational efficiency}

Neural and behavioral data analysis is most effective when computations are
performed quickly, ideally in real time. Slow computations discourage data
exploration and hinder scientific discovery. The large dataset sizes generated by
NaLoDuCo experimentation pose a significant challenge for fast data analysis.

To overcome this limitation, we will combine distributed and GPU computing.
%
Distributed computing is a paradigm
in which tasks and data are divided across multiple computers. Instead of
relying on a single powerful machine, distributed computing accelerates
processing by executing multiple parts of a computation in parallel.
%
GPU computing is a parallel computing approach that uses Graphics Processing
Units (GPUs) to accelerate computational tasks. Unlike traditional Central
Processing Units (CPUs), which execute a few complex operations sequentially,
GPUs consist of thousands of smaller cores optimized for executing many
operations simultaneously.

Distributed and GPU computing address different bottlenecks in large-scale
computation. GPUs are highly efficient at parallelizing operations within a
single machine. They excel at matrix operations and batch processing. However,
GPUs are limited by memory and cannot scale indefinitely when dealing with huge
datasets that exceed the GPU memory.
%
Distributed computing allows to split workload across multiple machines,
overcoming memory and computational limitations. It is particularly useful for
scaling to massive datasets (e.g., long-term time series recordings).

\begin{comment}

For distributed computing to deliver substantial speed improvements,
computations must be decomposable into independent parallel tasks. Due to their
serial dependencies, time series models are difficult to decomposed in this
manner.
%
Still, time series models can benefit from distributed computing
infrastructures.

While not
all algorithms naturally support such decomposition, in machine learning
distributed computing remains valuable even for non-parallelizable algorithms.
For instance, it enables efficient hyperparameter optimization, where multiple
algorithm configurations can be tested in parallel, significantly reducing
overall runtime.

% distributed computing for time series models

% parallel version of the Kalman filter

\end{comment}

We will develop accelerated implementations of all methods in the library of
methods to process NaLoDuCo experimental data
(Section~\ref{sec:offlineAnalysisMethods}). These implementations will use
JAX\footnote{\url{https://docs.jax.dev/}} for model learning, inference, and numerical computation, Apache
Spark\footnote{\url{https://spark.apache.org/}} or
Dask\footnote{\url{https://www.dask.org/}} to distribute pre-processing and
feature extraction, and Ray\footnote{\url{https://docs.ray.io/}} to distribute
machine learning and deep learning functionality.

Thunder~\cite{} is a library developed in 2014 to accelerate the analysis of
large scale neural data. It was pioneering by introducing the use of
distributed computing in neural data analysis.
%
Our library is different from Thunder in that, besides analyzing large scale
neural data, it processes continual recordings, and needs to overcome
non-stationarity problems.
%
In addition, it includes methods to characterize behavior, while
Thunder focuses on neural activity.
%
Finally, Thunder implements simpler methods assuming independent and
identically distributed data, while our library contains more sophisticated
time series ones.

